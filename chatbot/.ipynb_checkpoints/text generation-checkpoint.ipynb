{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "013e36d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,pickle,numpy as np,pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import LSTM,Dense,Activation,Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c2e0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def preproc_data(text_df,txt_size):\n",
    "    text = list(text_df[\"text\"].values)\n",
    "    joined_text = \" \".join(text)\n",
    "    partial_txt = joined_text[:txt_size].lower()\n",
    "\n",
    "\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    partial_txt = re.sub(r'http\\S+', '', partial_txt) #removing links\n",
    "    partial_txt = strip_all_entities(partial_txt)#no hashtags or @'s\n",
    "    partial_txt = emoji_pattern.sub(r'', partial_txt) # no emoji\n",
    "    partial_txt = re.sub(r'[^A-Za-z0-9 ]+', '', partial_txt)\n",
    "    \n",
    "    return partial_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0135cc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2579562 i have never seen a genuine person like you your hard work on trusted fixed games has really spok english announce messi need sugar daddy asap eng english we still have some work to do but followers will come good things come to those who wait  does anyone know where i can download eng app i m having trouble memories of leo subscribe like video share  watch  how you react when of all the photos of your incredible career someone chooses that one  both barcelona and psg are in pot 2 for the upcoming champions league draw this will protect messi and barcelona i followed when joined became a fan when played there and fell in love lmao chosen mfrs still owes us 55 million euros ffs   club don t even agree on this the ultimate best of subscribe like video share  its amazing the respect you get once you quit working for places like walmart target and restaurants all the leo messi s wife has left this beautiful and forceful message for her husband after the departure of the world so  last hugs\n"
     ]
    }
   ],
   "source": [
    "text_df = pd.read_csv('datasets/messi_tweets.csv')\n",
    "partial_txt = preproc_data(text_df,100000000)\n",
    "\n",
    "print(len(partial_txt),partial_txt[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "680d0e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens are(:100): ['i', 'have', 'never', 'seen', 'a', 'genuine', 'person', 'like', 'you', 'your', 'hard', 'work', 'on', 'trusted', 'fixed', 'games', 'has', 'really', 'spok', 'english', 'announce', 'messi', 'need', 'sugar', 'daddy', 'asap', 'eng', 'english', 'we', 'still', 'have', 'some', 'work', 'to', 'do', 'but', 'followers', 'will', 'come', 'good', 'things', 'come', 'to', 'those', 'who', 'wait', 'does', 'anyone', 'know', 'where', 'i', 'can', 'download', 'eng', 'app', 'i', 'm', 'having', 'trouble', 'memories', 'of', 'leo', 'subscribe', 'like', 'video', 'share', 'watch', 'how', 'you', 'react', 'when', 'of', 'all', 'the', 'photos', 'of', 'your', 'incredible', 'career', 'someone', 'chooses', 'that', 'one', 'both', 'barcelona', 'and', 'psg', 'are', 'in', 'pot', '2', 'for', 'the', 'upcoming', 'champions', 'league', 'draw', 'this', 'will', 'protect']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+\")                            \n",
    "tokens = tokenizer.tokenize(partial_txt)\n",
    "print(\"tokens are(:100):\",tokens[:100])\n",
    "uniq_tokens = np.unique(tokens)\n",
    "#print(\"uniq tokens are(:100):\\n\",uniq_tokens[:100])\n",
    "uniq_tokens_idx = {token: idx for idx,token in enumerate(uniq_tokens)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fada7cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 10 #number of words to look at for context\n",
    "input_words =[]\n",
    "next_words = []\n",
    "for i in range(len(tokens)-n_words):\n",
    "    input_words.append(tokens[i:i+n_words])\n",
    "    next_words.append(tokens[i+n_words])\n",
    "X =np.zeros((len(input_words),n_words,len(uniq_tokens)),dtype=bool)\n",
    "Y = np.zeros((len(next_words),len(uniq_tokens)),dtype=bool)\n",
    "\n",
    "for i,words in enumerate(input_words):\n",
    "    for j, word in enumerate(words):\n",
    "        X[i,j,uniq_tokens_idx[word]] = 1\n",
    "    Y[i,uniq_tokens_idx[next_words[i]]] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e2a8eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "91/91 [==============================] - 20s 168ms/step - loss: 6.6632 - accuracy: 0.0242\n",
      "Epoch 2/50\n",
      "91/91 [==============================] - 17s 184ms/step - loss: 6.4593 - accuracy: 0.0285\n",
      "Epoch 3/50\n",
      "91/91 [==============================] - 15s 169ms/step - loss: 6.3294 - accuracy: 0.0376\n",
      "Epoch 4/50\n",
      "91/91 [==============================] - 15s 165ms/step - loss: 6.0916 - accuracy: 0.0622\n",
      "Epoch 5/50\n",
      "91/91 [==============================] - 17s 188ms/step - loss: 5.8429 - accuracy: 0.0885\n",
      "Epoch 6/50\n",
      "91/91 [==============================] - 19s 204ms/step - loss: 5.5918 - accuracy: 0.1085\n",
      "Epoch 7/50\n",
      "91/91 [==============================] - 17s 186ms/step - loss: 5.3303 - accuracy: 0.1255\n",
      "Epoch 8/50\n",
      "91/91 [==============================] - 15s 170ms/step - loss: 5.0711 - accuracy: 0.1524\n",
      "Epoch 9/50\n",
      "91/91 [==============================] - 16s 178ms/step - loss: 4.7562 - accuracy: 0.1758\n",
      "Epoch 10/50\n",
      "91/91 [==============================] - 20s 220ms/step - loss: 4.4595 - accuracy: 0.2046\n",
      "Epoch 11/50\n",
      "91/91 [==============================] - 18s 197ms/step - loss: 4.1394 - accuracy: 0.2428\n",
      "Epoch 12/50\n",
      "91/91 [==============================] - 16s 171ms/step - loss: 3.8263 - accuracy: 0.2814\n",
      "Epoch 13/50\n",
      "91/91 [==============================] - 16s 171ms/step - loss: 3.4986 - accuracy: 0.3277\n",
      "Epoch 14/50\n",
      "91/91 [==============================] - 16s 170ms/step - loss: 3.1810 - accuracy: 0.3724\n",
      "Epoch 15/50\n",
      "91/91 [==============================] - 16s 174ms/step - loss: 2.8676 - accuracy: 0.4177\n",
      "Epoch 16/50\n",
      "91/91 [==============================] - 15s 167ms/step - loss: 2.5719 - accuracy: 0.4767\n",
      "Epoch 17/50\n",
      "91/91 [==============================] - 15s 166ms/step - loss: 2.2924 - accuracy: 0.5300\n",
      "Epoch 18/50\n",
      "91/91 [==============================] - 16s 174ms/step - loss: 2.0080 - accuracy: 0.5838\n",
      "Epoch 19/50\n",
      "91/91 [==============================] - 15s 167ms/step - loss: 1.7646 - accuracy: 0.6320\n",
      "Epoch 20/50\n",
      "91/91 [==============================] - 15s 165ms/step - loss: 1.5284 - accuracy: 0.6912\n",
      "Epoch 21/50\n",
      "91/91 [==============================] - 16s 172ms/step - loss: 1.3111 - accuracy: 0.7439\n",
      "Epoch 22/50\n",
      "91/91 [==============================] - 16s 175ms/step - loss: 1.1344 - accuracy: 0.7911\n",
      "Epoch 23/50\n",
      "91/91 [==============================] - 16s 173ms/step - loss: 0.9554 - accuracy: 0.8362\n",
      "Epoch 24/50\n",
      "91/91 [==============================] - 16s 175ms/step - loss: 0.7948 - accuracy: 0.8758\n",
      "Epoch 25/50\n",
      "91/91 [==============================] - 16s 173ms/step - loss: 0.6683 - accuracy: 0.9029\n",
      "Epoch 26/50\n",
      "91/91 [==============================] - 15s 170ms/step - loss: 0.5616 - accuracy: 0.9239\n",
      "Epoch 27/50\n",
      "91/91 [==============================] - 16s 173ms/step - loss: 0.4635 - accuracy: 0.9434\n",
      "Epoch 28/50\n",
      "91/91 [==============================] - 17s 184ms/step - loss: 0.3938 - accuracy: 0.9506\n",
      "Epoch 29/50\n",
      "91/91 [==============================] - 15s 170ms/step - loss: 0.3209 - accuracy: 0.9630\n",
      "Epoch 30/50\n",
      "91/91 [==============================] - 15s 170ms/step - loss: 0.2753 - accuracy: 0.9690\n",
      "Epoch 31/50\n",
      "91/91 [==============================] - 17s 190ms/step - loss: 0.2284 - accuracy: 0.9727\n",
      "Epoch 32/50\n",
      "91/91 [==============================] - 16s 179ms/step - loss: 0.2138 - accuracy: 0.9728\n",
      "Epoch 33/50\n",
      "91/91 [==============================] - 17s 182ms/step - loss: 0.1790 - accuracy: 0.9768\n",
      "Epoch 34/50\n",
      "91/91 [==============================] - 16s 176ms/step - loss: 0.1651 - accuracy: 0.9766\n",
      "Epoch 35/50\n",
      "91/91 [==============================] - 16s 181ms/step - loss: 0.1633 - accuracy: 0.9750\n",
      "Epoch 36/50\n",
      "91/91 [==============================] - 16s 174ms/step - loss: 0.1359 - accuracy: 0.9817\n",
      "Epoch 37/50\n",
      "91/91 [==============================] - 16s 172ms/step - loss: 0.1446 - accuracy: 0.9772\n",
      "Epoch 38/50\n",
      "91/91 [==============================] - 16s 171ms/step - loss: 0.1414 - accuracy: 0.9758\n",
      "Epoch 39/50\n",
      "91/91 [==============================] - 16s 174ms/step - loss: 0.1262 - accuracy: 0.9805\n",
      "Epoch 40/50\n",
      "91/91 [==============================] - 16s 178ms/step - loss: 0.1247 - accuracy: 0.9804\n",
      "Epoch 41/50\n",
      "91/91 [==============================] - 16s 181ms/step - loss: 0.1104 - accuracy: 0.9823\n",
      "Epoch 42/50\n",
      "91/91 [==============================] - 16s 177ms/step - loss: 0.1058 - accuracy: 0.9824\n",
      "Epoch 43/50\n",
      "91/91 [==============================] - 17s 182ms/step - loss: 0.1000 - accuracy: 0.9839\n",
      "Epoch 44/50\n",
      "91/91 [==============================] - 16s 171ms/step - loss: 0.1039 - accuracy: 0.9823\n",
      "Epoch 45/50\n",
      "91/91 [==============================] - 16s 171ms/step - loss: 0.1021 - accuracy: 0.9835\n",
      "Epoch 46/50\n",
      "91/91 [==============================] - 16s 179ms/step - loss: 0.1050 - accuracy: 0.9814\n",
      "Epoch 47/50\n",
      "91/91 [==============================] - 16s 174ms/step - loss: 0.0919 - accuracy: 0.9850\n",
      "Epoch 48/50\n",
      "91/91 [==============================] - 16s 172ms/step - loss: 0.1051 - accuracy: 0.9800\n",
      "Epoch 49/50\n",
      "91/91 [==============================] - 17s 183ms/step - loss: 0.0988 - accuracy: 0.9818\n",
      "Epoch 50/50\n",
      "91/91 [==============================] - 17s 190ms/step - loss: 0.0971 - accuracy: 0.9810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2451feee990>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original\n",
    "shape = (n_words,len(uniq_tokens))\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128,input_shape=shape,return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(uniq_tokens)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=RMSprop(learning_rate=0.01),metrics=[\"accuracy\"])\n",
    "model.fit(X,Y,batch_size=128,epochs=50,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a64f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#odel.save(\"mymodel.h5\")\n",
    "model = load_model(\"mymodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcc85841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(input_txt,n_best):\n",
    "    input_txt=input_txt.lower()\n",
    "    X=np.zeros((1,n_words,len(uniq_tokens)))\n",
    "    for i,word in enumerate(input_txt.split()):\n",
    "        X[0,1,uniq_tokens_idx[word]] = 1\n",
    "    predictions = model.predict(X)[0]\n",
    "    return np.argpartition(predictions,-n_best)[-n_best:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5311d675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "['imagine', 'came', 'supposedly', 'when', 'everyone']\n"
     ]
    }
   ],
   "source": [
    "possible = predict_next_word(\"I love messi so much cause he is the best player\",5)\n",
    "print([uniq_tokens[idx] for idx in possible])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7c4c44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what do u know about messi strange separate purity someones font era right list introduced loves hack organisers mason board tottenham stream perspective secondly bitches trying went gi lifetime difficult som insta possible nigh politics links 6 cher someones 16 georgia 900 admin choose exit collection i helpers 13 confront august alert sterile share worried taxes bril losses legit outchea heading nigeria jordan ashamed ends because actually base paris benefits sucks departure shopping joy offer grateful subsidy khnh race felt media 3pm again n constant couple inspiration question blockchain described since screw world villa cristiano confront reviews k shattered blockchain disgusting proves not mans wait pictures'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_txt(input_txt,text_length,creativity=3):\n",
    "    word_sequence = input_txt.split()\n",
    "    current=0\n",
    "    for _ in range(text_length):\n",
    "        sub_sequence=\" \".join(tokenizer.tokenize(\" \".join(word_sequence).lower())[current:current+n_words])\n",
    "        try:\n",
    "            choice = np.random.choice(uniq_tokens)\n",
    "        except:\n",
    "            choice = random.choice(uniq_tokens)\n",
    "        word_sequence.append(choice)\n",
    "        current+=1\n",
    "    return \" \".join(word_sequence)\n",
    "generate_txt(\"what do u know about messi\",100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337fe4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f0f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428b601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
